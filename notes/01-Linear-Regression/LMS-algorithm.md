在介绍线性回归方程之前简单提一下监督学习，线性回归是监督学习的一种，监督学习就是通过一些训练集得到一个优化模型*h*，通过这个模型对新的输入变量x得到输出变量y，监督学习就是为了得到一个最理想的*h*模型。

 <img src="../../images/01/supervised.jpg" width = "60%"/>

# **线性回归方程**

在上面提到过优化模型*h*可以看成是输入*x*的函数，为了方便起见记为<img src="../../images/common/h(x).jpg" width = "2%"/>，并假设该方程是关于*x*的连续函数，这是监督学习最为简单的一种。具体表达式如下所示：

<img src="../../images/01/ehtaX.jpg" width = "60%"/>

线性回归方程的目的就是使得<img src="../../images/common/h(x).jpg" width = "1.5%"/>尽可能的逼近于*y*，为了便于表示这种近似关系，定义误差方程<img src="../../images/common/j.jpg" width = "1.5%"/>:

<img src="../../images/01/j2.jpg" width = "40%"/>

上述的表达式类似于最小均方值误差求解的方法，但是后面会给出数学上一般形式的表达方式。先做一个小结，上面主要讨论了在输入一些变量后求得的估计函数可以使后续的输入变量更加的接近真实值，这就是监督学习最简单的一种形式。

## **最小均方误差算法LMS**

LMS算法的目的就是如何选择权重<img src="../../images/common/ehta.jpg" width = "1.5%"/>使得<img src="../../images/common/j.jpg" width = "1.5%"/>达到最小值。
